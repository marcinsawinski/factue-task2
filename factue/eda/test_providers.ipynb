{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from factue.methods.llms_langchain.lms import init_lms, get_models\n",
    "from factue.methods.llms_langchain.openai import init_openai, get_openai_models\n",
    "from factue.methods.llms_langchain.azure_openai import init_azure_openai\n",
    "from factue.methods.llms_langchain.ollama import init_ollama, get_ollama_models\n",
    "# from factue.methods.llms_langchain.vllm import init_vllm\n",
    "\n",
    "def test_provider(llm_generator,modes, prompt):\n",
    "    for mode in modes:\n",
    "        print(mode, llm_generator, \"*\"*50)\n",
    "        llm =llm_generator(mode=mode)\n",
    "        if mode == \"llm\":\n",
    "            print(llm.invoke(prompt))\n",
    "        elif mode == \"chat\":\n",
    "            print(llm.invoke(prompt).content)\n",
    "        elif mode == \"embedding\":\n",
    "            print(llm.embed_query(prompt))\n",
    "\n",
    "\n",
    "modes = [\"llm\", \"chat\", \"embeddings\"]\n",
    "prompt = '2 +2 = '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm <function init_lms at 0x107e3fe20> **************************************************\n",
      "4, so the next number is 4.\n",
      "\n",
      "Wait, but in the problem statement, it's said that each number is obtained by adding 2 to the previous one. So starting from 1, next is 3, then 5, etc., which are odd numbers. But in the problem statement, it's said that each number is obtained by adding 2 to the previous one. So starting from 1, next is 3, then 5, etc., which are odd numbers. But in the problem statement, it's said that each number is obtained by adding 2 to the previous one. So starting from 1, next is 3, then 5, etc., which are odd numbers. So the sequence is 1, 3, 5, 7, etc.\n",
      "\n",
      "But in the problem statement, it's said that each number is obtained by adding 2 to the previous one. So starting from 1, next is 3, then 5, etc., which are odd numbers. So the sequence is 1, 3, 5, 7, etc.\n",
      "\n",
      "But in the problem statement, it's said that each number is obtained by adding 2 to the previous one. So starting from \n",
      "chat <function init_lms at 0x107e3fe20> **************************************************\n",
      "First, I recognize that the user has provided a simple mathematical expression: 2 + 2.\n",
      "\n",
      "To solve this, I'll start by identifying the numbers involved in the equation. Here, both operands are 2.\n",
      "\n",
      "Next, I'll perform the addition operation by summing these two numbers. Adding 2 and 2 gives me 4.\n",
      "\n",
      "Therefore, the result of the expression 2 + 2 is 4.\n",
      "</think>\n",
      "\n",
      "**Solution:**\n",
      "\n",
      "We are given the expression:\n",
      "\\[ 2 + 2 \\]\n",
      "\n",
      "To solve for this, follow these steps:\n",
      "\n",
      "1. **Identify the numbers to add:**\n",
      "   \\[ 2 \\quad \\text{and} \\quad 2 \\]\n",
      "\n",
      "2. **Perform the addition:**\n",
      "   \\[ 2 + 2 = 4 \\]\n",
      "\n",
      "**Final Answer:**\n",
      "\\[\n",
      "\\boxed{4}\n",
      "\\]\n",
      "embeddings <function init_lms at 0x107e3fe20> **************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages/langchain_openai/embeddings/base.py:269: UserWarning: WARNING! temperature is not default parameter.\n",
      "                    temperature was transferred to model_kwargs.\n",
      "                    Please confirm that temperature is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_provider(init_lms,modes, prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# llm = init_lms(mode=mode)\n",
    "# llm = init_ollama(mode=mode)\n",
    "# llm = init_openai(mode=mode)\n",
    "# llm = init_azure(mode=mode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
