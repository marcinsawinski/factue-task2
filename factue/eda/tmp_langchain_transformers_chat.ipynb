{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77f27879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/mnt/openfact/users/msawinski/factue-task2')\n",
    "# from factue.methods.llm_langchain.llm import Llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b1a40d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7510fb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16220b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf192ec40a44f7891d2a450f26628c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer first\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\", \n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Fix pad_token if needed\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=quantization_config,\n",
    "    # device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Build HF pipeline\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    # temperature=0.0,\n",
    "    repetition_penalty=1.03,\n",
    "    return_full_text=False,\n",
    ")\n",
    "\n",
    "# Now build LangChain LLM\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "chat_model = ChatHuggingFace(llm=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d67696b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=\"float16\",\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "# )\n",
    "\n",
    "# llm = HuggingFacePipeline.from_model_id(\n",
    "#     model_id=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "#     task=\"text-generation\",\n",
    "    \n",
    "#     pipeline_kwargs=dict(\n",
    "#         max_new_tokens=512,\n",
    "#         do_sample=False,\n",
    "#         repetition_penalty=1.03,\n",
    "#         return_full_text=False,\n",
    "#         tokenizer=tokenizer,\n",
    "#     ),\n",
    "#     model_kwargs={\"quantization_config\": quantization_config},\n",
    "# )\n",
    "\n",
    "# chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a72dc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/worker/notebooks/env_py311_ms_guidance/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/worker/notebooks/env_py311_ms_guidance/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"rating\": 0,\n",
      "  \"reason\": \"The claim does not mention the Karnofsky family or the boy's name, but rather shows a photo of Louis Armstrong as a child.\",\n",
      "  \"alternative\": \"The Karnofsky Jewish family, who immigrated to the United States from Lithuania, employed a 7-year-old boy and adopted him into their home.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "promt_format = \"\"\"\\nOutput ONLY JSON: in the format {{\"rating\": 0-10, \"reason\": \"reason for rating\",\"alternative\": \"claim text that better represents the text of post}}.\"\"\"\n",
    "\n",
    "prmpt_system = \"\"\"You are an expert journalist. Your task is to validate if a claim was accuratley extracted from a post. Compare it and determine how accurately the claim reflects post. You outout rating  from 0 to 10, where 0 means the claim is completely different from original text and 10 means the claim most accurately reflects the post. Do not include additional information. Compare ONLY the claim text with post text.\"\"\"\n",
    "\n",
    "\n",
    "claim = \"\"\"Photo shows Louis Armstrong as a child\"\"\"\n",
    "post = \"\"\"The Karnofsky Jewish family, who immigrated to the United States from Lithuania, employed a 7-year-old boy and adopted (so to speak) him into their home.  He was originally given homework to get food because he was a starving kid.  He remained under the Jewish families employ, until he was 12  Karnofsky gave him money to buy his first instrument, which was a common instrument in Jewish families.  They really admired his musical talent.Later, when he became a professional\"\"\"\n",
    "prmpt_user = f\"\"\"POST:\\n{post}\\n\\nCLAIM:\\n{claim}\"\"\"\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", prmpt_system+promt_format),\n",
    "        (\"human\", prmpt_user),\n",
    "    ]\n",
    ")\n",
    "prompt = prompt_template.invoke(\n",
    "    {\n",
    "        \"post\": post,\n",
    "        \"claim\": claim\n",
    "    })\n",
    "print(chat_model.invoke(prompt).content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python MS (guidance)",
   "language": "python",
   "name": "env_py311_ms_guidance"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
