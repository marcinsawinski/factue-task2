{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ce6dbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m ipykernel install --user --name env_py311_ms_guidance  --display-name \"Python MS (guidance)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb52f44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43aa3f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0720ae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",  # nf4 = Normal Float 4-bit (better quality)\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=\"bfloat16\",  # or \"float16\"\n",
    ")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"  # Replace with your model path or Hugging Face repo\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10a03db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from guidance import models, gen, select\n",
    "from guidance import system, user, assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7184fad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc82f2fb71164df7a08184c37e362f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/worker/notebooks/env_py311_ms_guidance/lib/python3.11/site-packages/guidance/models/_transformers.py:162: UserWarning: Could not build_byte tokens from the tokenizer by encoding token strings: Round-trip encoding of tokens [!] failed! Got [128000, 0]\n",
      "  warnings.warn(\n",
      "/home/worker/notebooks/env_py311_ms_guidance/lib/python3.11/site-packages/guidance/chat.py:80: UserWarning: Chat template {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- set date_string = \"26 Jul 2024\" %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message + builtin tools #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if builtin_tools is defined or tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{%- if builtin_tools is defined %}\n",
      "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
      "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
      "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
      "                {%- if not loop.last %}\n",
      "                    {{- \", \" }}\n",
      "                {%- endif %}\n",
      "                {%- endfor %}\n",
      "            {{- \")\" }}\n",
      "        {%- else  %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "            {{- '\"parameters\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- \"}\" }}\n",
      "        {%- endif %}\n",
      "        {%- if builtin_tools is defined %}\n",
      "            {#- This means we're in ipython mode #}\n",
      "            {{- \"<|eom_id|>\" }}\n",
      "        {%- else %}\n",
      "            {{- \"<|eot_id|>\" }}\n",
      "        {%- endif %}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      " was unable to be loaded directly into guidance.\n",
      "                        Defaulting to the ChatML format which may not be optimal for the selected model. \n",
      "                        For best results, create and pass in a `guidance.ChatTemplate` subclass for your model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7493f847eb4f078d396351028eb35c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/worker/notebooks/env_py311_ms_guidance/lib/python3.11/site-packages/guidance/metrics/_metrics.py\", line 199, in _monitor_fn\n",
      "    gpu_stats = gpustat.GPUStatCollection.new_query()\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/worker/notebooks/env_py311_ms_guidance/lib/python3.11/site-packages/gpustat/core.py\", line 603, in new_query\n",
      "    gpu_info = get_gpu_info(handle)\n",
      "               ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/worker/notebooks/env_py311_ms_guidance/lib/python3.11/site-packages/gpustat/core.py\", line 555, in get_gpu_info\n",
      "    time.sleep(0.1)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import guidance\n",
    "from guidance import models, gen, select\n",
    "\n",
    "# Wrap the pipeline with Guidance\n",
    "llm = guidance.models.Transformers(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0baebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm = llm + 'I like the color ' + select(['red', 'blue', 'green'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bef5d3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm = llm+ \"What is the capital of France?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c84f1e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm + gen(max_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c797a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with system():\n",
    "#     lm = llm + \"You are a helpful assistant.\"\n",
    "\n",
    "# with user():\n",
    "#     lm += \"What is the meaning of life?\"\n",
    "\n",
    "# with assistant():\n",
    "#     lm += gen(\"response\", max_tokens=200,stop=\"<|im_end|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a72dc21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cbb36f09abe4b98bf59c7d887f06304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "promt_format = \"\"\"\\nOutput ONLY JSON: in the format {{\"rating\": 0-10, \"reason\": \"reason for rating\",\"alternative\": \"claim text that better represents the text of post}}.\"\"\"\n",
    "\n",
    "prmpt_system = \"\"\"You are an expert journalist. Your task is to validate if a claim was accuratley extracted from a post. Compare it and determine how accurately the claim reflects post. You outout rating  from 0 to 10, where 0 means the claim is completely different from original text and 10 means the claim most accurately reflects the post. Do not include additional information. Compare ONLY the claim text with post text.\"\"\"\n",
    "\n",
    "\n",
    "claim = \"\"\"Photo shows Louis Armstrong as a child\"\"\"\n",
    "post = \"\"\"The Karnofsky Jewish family, who immigrated to the United States from Lithuania, employed a 7-year-old boy and adopted (so to speak) him into their home.  He was originally given homework to get food because he was a starving kid.  He remained under the Jewish families employ, until he was 12  Karnofsky gave him money to buy his first instrument, which was a common instrument in Jewish families.  They really admired his musical talent.Later, when he became a professional\"\"\"\n",
    "prmpt_user = f\"\"\"POST:\\n{post}\\n\\nCLAIM:\\n{claim}\"\"\"\n",
    "\n",
    "with system():\n",
    "    lm = llm + prmpt_system + promt_format\n",
    "\n",
    "with user():\n",
    "    lm += prmpt_user\n",
    "\n",
    "with assistant():\n",
    "    lm += gen(\"response\", max_tokens=200, temperature=0.0, stop=\"<|im_end|>\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python MS (guidance)",
   "language": "python",
   "name": "env_py311_ms_guidance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
