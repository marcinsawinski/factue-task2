{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7bc506d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (0.3.23)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.56 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from langchain_community) (0.3.56)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.24 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from langchain_community) (0.3.24)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from langchain_community) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from langchain_community) (3.11.18)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from langchain_community) (8.5.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from langchain_community) (2.9.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from langchain_community) (0.3.34)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from langchain_community) (2.2.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from langchain<1.0.0,>=0.3.24->langchain_community) (0.3.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from langchain<1.0.0,>=0.3.24->langchain_community) (2.11.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.56->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.56->langchain_community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.56->langchain_community) (4.13.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.56->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
      "Requirement already satisfied: anyio in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.9.0)\n",
      "Requirement already satisfied: certifi in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain_community) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain_community) (0.4.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain_community) (2.4.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/marcinsawinski/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "849d82fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models.azureml_endpoint import AzureMLChatOnlineEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2bc877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_url = 'https://nonoai-swe-beimq.swedencentral.inference.ml.azure.com/score'\n",
    "endpoint_api_key = '6kTAs6suMkug3ZsiAEyROnGjAMDzkKw4FeXZu2QpMgw1HYFd1iU4JQQJ99BEAAAAAAAAAAAAINFRAZML2Bou'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9a8f28b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'[{\"0\":\"My name is Tom and I like to be called Tom. I am a fun loving, happy guy who loves to travel and meet new people. I am very friendly and sociable. I love being outdoors, playing sport and being with friends. I love to have a laugh and I like to be able to make others laugh too. I am very down to earth and very honest. I believe in treating others how you would like to be treated. I love to learn new things and I am always keen to try new things.\\\\nI am looking for a girl who is friendly and likes to have fun. I am looking for a girl who is outgoing and loves to have fun. I am looking for a girl who is down to earth and honest. I am looking for a girl who is caring and loving. I am looking for a girl who is adventurous and loves to travel. I am looking for a girl who is open minded and loves to try new things. I am looking for a girl who is not afraid to be herself and is\"}]'\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "\n",
    "# Request data goes here\n",
    "# The example below assumes JSON formatting which may be updated\n",
    "# depending on the format your endpoint expects.\n",
    "# More information can be found here:\n",
    "# https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script\n",
    "data = {\n",
    "  \"input_data\": {\n",
    "    \"input_string\": [\n",
    "      \"My name is Tom and I like to\"\n",
    "    ],\n",
    "    \"parameters\": {\n",
    "      \"temperature\": 0.8,\n",
    "      \"top_p\": 0.8,\n",
    "      \"max_new_tokens\": 200\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "body = str.encode(json.dumps(data))\n",
    "\n",
    "url = 'https://nonoai-swe-beimq.swedencentral.inference.ml.azure.com/score'\n",
    "# Replace this with the primary/secondary key, AMLToken, or Microsoft Entra ID token for the endpoint\n",
    "api_key = endpoint_api_key\n",
    "if not api_key:\n",
    "    raise Exception(\"A key should be provided to invoke the endpoint\")\n",
    "\n",
    "\n",
    "headers = {'Content-Type':'application/json', 'Accept': 'application/json', 'Authorization':('Bearer '+ api_key)}\n",
    "\n",
    "req = urllib.request.Request(url, body, headers)\n",
    "\n",
    "try:\n",
    "    response = urllib.request.urlopen(req)\n",
    "\n",
    "    result = response.read()\n",
    "    print(result)\n",
    "except urllib.error.HTTPError as error:\n",
    "    print(\"The request failed with status code: \" + str(error.code))\n",
    "\n",
    "    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
    "    print(error.info())\n",
    "    print(error.read().decode(\"utf8\", 'ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e1aa9595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"0\":\"System: You are a helpful assistant.\\nUser: My name is Tom and I like to\\nAssistant: What is your name, Tom?\\nAssistant: What do you like to do, Tom?\\nAssistant: Can you tell me more about your interests?\\nAssistant: I'm sorry, I didn't quite get that. Can you try again?\\nUser: I enjoy playing sports and spending time with friends.\\nAssistant: That's great to hear, Tom. I love playing sports too. What sports do you like to play?\\nAssistant: Can you tell me more about your favorite sports?\\nAssistant: That's great, Tom. I also enjoy playing tennis. Do you have any other hobbies?\\nAssistant: That's great, Tom. I also enjoy playing tennis. Do you have any other hobbies?\\nUser: I also like to read and watch movies.\\nAssistant: I'm sorry, I didn't quite get that. Can you try again?\\nUser: I also like to read and watch movies.\\nAssistant: That's great, Tom. I also enjoy reading and watching movies. What are your favorite books and movies\"}]\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "\n",
    "# Simulate chat format as a plain text prompt\n",
    "chat_prompt = (\n",
    "    \"System: You are a helpful assistant.\\n\"\n",
    "    \"User: My name is Tom and I like to\\n\"\n",
    "    \"Assistant:\"\n",
    ")\n",
    "\n",
    "data = {\n",
    "    \"input_data\": {\n",
    "        \"input_string\": [chat_prompt],\n",
    "        \"parameters\": {\n",
    "            \"temperature\": 0.8,\n",
    "            \"top_p\": 0.8,\n",
    "            \"max_new_tokens\": 200\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "body = str.encode(json.dumps(data))\n",
    "\n",
    "url = 'https://nonoai-swe-beimq.swedencentral.inference.ml.azure.com/score'\n",
    "api_key = endpoint_api_key\n",
    "if not api_key:\n",
    "    raise Exception(\"A key should be provided to invoke the endpoint\")\n",
    "\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Accept': 'application/json',\n",
    "    'Authorization': 'Bearer ' + api_key\n",
    "}\n",
    "\n",
    "req = urllib.request.Request(url, body, headers)\n",
    "\n",
    "try:\n",
    "    response = urllib.request.urlopen(req)\n",
    "    result = response.read()\n",
    "    print(result.decode(\"utf-8\"))\n",
    "except urllib.error.HTTPError as error:\n",
    "    print(\"The request failed with status code: \" + str(error.code))\n",
    "    print(error.info())\n",
    "    print(error.read().decode(\"utf8\", 'ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9414c05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.language_models.llms import LLM\n",
    "from typing import List, Optional, Any\n",
    "import urllib.request\n",
    "import json\n",
    "\n",
    "class AzureLlamaLLM(LLM):\n",
    "    url: str\n",
    "    api_key: str\n",
    "    temperature: float = 0.8\n",
    "    top_p: float = 0.8\n",
    "    max_new_tokens: int = 4096\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        data = {\n",
    "            \"input_data\": {\n",
    "                \"input_string\": [prompt],\n",
    "                \"parameters\": {\n",
    "                    \"temperature\": self.temperature,\n",
    "                    \"top_p\": self.top_p,\n",
    "                    \"max_new_tokens\": self.max_new_tokens\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        body = str.encode(json.dumps(data))\n",
    "\n",
    "        headers = {\n",
    "            'Content-Type': 'application/json',\n",
    "            'Accept': 'application/json',\n",
    "            'Authorization': f'Bearer {self.api_key}'\n",
    "        }\n",
    "\n",
    "        req = urllib.request.Request(self.url, body, headers)\n",
    "        try:\n",
    "            response = urllib.request.urlopen(req)\n",
    "            result = json.loads(response.read())\n",
    "            # Step 1: Decode the bytes\n",
    "            decoded = result[0].decode(\"utf-8\")\n",
    "\n",
    "            # Step 2: Parse the JSON\n",
    "            parsed = json.loads(decoded)\n",
    "\n",
    "            # Step 3: Extract the string\n",
    "            text = parsed[0][\"0\"]\n",
    "\n",
    "            return text\n",
    "\n",
    "        except urllib.error.HTTPError as error:\n",
    "            print(\"Request failed:\", error.code)\n",
    "            print(error.info())\n",
    "            print(error.read().decode(\"utf8\", 'ignore'))\n",
    "            raise error\n",
    "    \n",
    "    # def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "    #     data = {\n",
    "    #         \"input_data\": {\n",
    "    #             \"input_string\": [prompt],\n",
    "    #             \"parameters\": {\n",
    "    #                 \"temperature\": self.temperature,\n",
    "    #                 \"top_p\": self.top_p,\n",
    "    #                 \"max_new_tokens\": self.max_new_tokens\n",
    "    #             }\n",
    "    #         }\n",
    "    #     }\n",
    "\n",
    "    #     body = str.encode(json.dumps(data))\n",
    "\n",
    "    #     headers = {\n",
    "    #         'Content-Type': 'application/json',\n",
    "    #         'Accept': 'application/json',\n",
    "    #         'Authorization': f'Bearer {self.api_key}'\n",
    "    #     }\n",
    "\n",
    "    #     req = urllib.request.Request(self.url, body, headers)\n",
    "    #     try:\n",
    "    #         response = urllib.request.urlopen(req)\n",
    "    #         result = json.loads(response.read())\n",
    "    #         return result # Adjust key if your API returns a different structure\n",
    "    #     except urllib.error.HTTPError as error:\n",
    "    #         print(\"Request failed:\", error.code)\n",
    "    #         print(error.info())\n",
    "    #         print(error.read().decode(\"utf8\", 'ignore'))\n",
    "    #         raise error\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"azure-llama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa6da6df",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy name is Julien and I like to \u001b[39m\u001b[38;5;132;01m{hobby}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m chain \u001b[38;5;241m=\u001b[39m LLMChain(llm\u001b[38;5;241m=\u001b[39mllm, prompt\u001b[38;5;241m=\u001b[39mprompt)\n\u001b[0;32m---> 12\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhobby\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgo hiking in the Alps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[0;32m~/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages/langchain/chains/base.py:167\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    166\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    168\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages/langchain/chains/base.py:157\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    156\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 157\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    160\u001b[0m     )\n\u001b[1;32m    162\u001b[0m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    163\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages/langchain/chains/llm.py:127\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    124\u001b[0m     inputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    125\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    126\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 127\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages/langchain/chains/llm.py:139\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    137\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[1;32m    147\u001b[0m         cast(\u001b[38;5;28mlist\u001b[39m, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[1;32m    148\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:764\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    762\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    763\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:971\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    957\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    958\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    959\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    969\u001b[0m         )\n\u001b[1;32m    970\u001b[0m     ]\n\u001b[0;32m--> 971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    979\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    980\u001b[0m         callback_managers[idx]\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    981\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[1;32m    989\u001b[0m     ]\n",
      "File \u001b[0;32m~/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:790\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    781\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    787\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    788\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    789\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 790\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    794\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    797\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    798\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    799\u001b[0m         )\n\u001b[1;32m    800\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    801\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:1547\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1542\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m   1544\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1545\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1546\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m-> 1547\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1548\u001b[0m     )\n\u001b[1;32m   1549\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m   1550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "Cell \u001b[0;32mIn[29], line 37\u001b[0m, in \u001b[0;36mAzureLlamaLLM._call\u001b[0;34m(self, prompt, stop)\u001b[0m\n\u001b[1;32m     35\u001b[0m result \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Step 1: Decode the bytes\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Step 2: Parse the JSON\u001b[39;00m\n\u001b[1;32m     40\u001b[0m parsed \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(decoded)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = AzureLlamaLLM(\n",
    "    url=\"https://nonoai-swe-beimq.swedencentral.inference.ml.azure.com/score\",\n",
    "    api_key=endpoint_api_key\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"My name is Julien and I like to {hobby}.\")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "response = chain.invoke(input = {\"hobby\":\"go hiking in the Alps\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40208aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from langchain_community.chat_models.azureml_endpoint import (\n",
    "#     AzureMLEndpointApiType,\n",
    "#     CustomOpenAIChatContentFormatter,\n",
    "# )\n",
    "# from langchain_core.messages import HumanMessage\n",
    "\n",
    "# chat = AzureMLChatOnlineEndpoint(\n",
    "#     endpoint_url=endpoint_url,\n",
    "#     endpoint_api_type=AzureMLEndpointApiType.dedicated,\n",
    "#     endpoint_api_key=endpoint_api_key,\n",
    "#     content_formatter=CustomOpenAIChatContentFormatter(),\n",
    "# )\n",
    "# response = chat.invoke(\n",
    "#     [HumanMessage(content=\"Will the Collatz conjecture ever be solved?\")]\n",
    "# )\n",
    "# response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
