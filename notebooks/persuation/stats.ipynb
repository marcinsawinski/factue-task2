{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc303d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = '/mnt/openfact/users/msawinski/factue-task2'\n",
    "# wd = '/Users/marcinsawinski/Documents/GitHub/factue-task2'\n",
    "import sys, os\n",
    "os.chdir(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "baa931db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from factue.methods.llm_calls import load_template_list\n",
    "for x in load_template_list(job='persuasion', step=\"detect\", prompt_version='v001').keys():\n",
    "    print(f'\"{x}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "babd803f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "root = Path(\"data/llm_output/persuasion\")\n",
    "# Example path template\n",
    "files = root.rglob(\"*/*.parquet\")\n",
    "\n",
    "# Read all files and add file path as a column\n",
    "df_list = []\n",
    "for f in files:\n",
    "    df_part = pd.read_parquet(f)\n",
    "    df_part['source_file'] = f  # add the file path\n",
    "    df_list.append(df_part)\n",
    "\n",
    "# Combine into one DataFrame\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "df['split'] = df.source_file.astype(str).str.split('/',expand=True)[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb00c039",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e306ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pred.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "657dc767",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.split=='train'].groupby([\"prompt_name\",\"text_lang\",\"model_name\",\"split\"])['filename'].agg(\"count\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bb44b23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_binary(x):\n",
    "    return 1 if str(x).strip().lower() in {'1', 'true'} else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c3da0615",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['source_file'].value_counts().sort_index()\n",
    "df['gold'] = df['gold'].apply(normalize_binary)\n",
    "df['pred'] = df['pred'].apply(normalize_binary)\n",
    "df['split'] = df.source_file.astype(str).str.split('/',expand=True)[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9ae9f572",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.prompt_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "228d066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "# Define a function to compute metrics for a group\n",
    "# Assume df has columns: prompt_name, text_lang, gold, pred\n",
    "rows = []\n",
    "\n",
    "def is_valid_label(x):\n",
    "    return x in (0, 1)\n",
    "\n",
    "rows = []\n",
    "\n",
    "for prompt_name in df['prompt_name'].unique():\n",
    "    for text_lang in df.loc[df['prompt_name'] == prompt_name, 'text_lang'].unique():\n",
    "        for model_name in df.loc[(df['prompt_name'] == prompt_name) & (df['text_lang'] == text_lang), 'model_name'].unique():\n",
    "            group = df[\n",
    "                (df['prompt_name'] == prompt_name) &\n",
    "                (df['text_lang'] == text_lang) &\n",
    "                (df['model_name'] == model_name)\n",
    "            ]\n",
    "            if len(group) > 0:\n",
    "                row = {\n",
    "                    'prompt_name': prompt_name,\n",
    "                    'text_lang': text_lang,\n",
    "                    'model_name': model_name,\n",
    "                    'accuracy': accuracy_score(group['gold'], group['pred']),\n",
    "                    'precision': precision_score(group['gold'], group['pred'], zero_division=0),\n",
    "                    'recall': recall_score(group['gold'], group['pred'], zero_division=0),\n",
    "                    'f1': f1_score(group['gold'], group['pred'], zero_division=0),\n",
    "                    'support': len(group)\n",
    "                }\n",
    "                rows.append(row)\n",
    "\n",
    "results = pd.DataFrame(rows)\n",
    "results[['accuracy', 'precision','recall', 'f1']] = results[['accuracy', 'precision','recall', 'f1']].round(2)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "879f3744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ef2ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(metrics), 1, figsize=(10, 18), sharex=True)\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    sns.barplot(\n",
    "        data=results,\n",
    "        y='technique_id',\n",
    "        x=metric,\n",
    "        hue='text_lang',\n",
    "        ax=ax,\n",
    "        palette='muted',\n",
    "        errorbar=None  # fallback for compatibility\n",
    "    )\n",
    "    ax.set_title(metric.capitalize())\n",
    "    ax.set_xlim(0, 1.05)\n",
    "    ax.set_ylabel('Technique ID')\n",
    "    ax.set_xlabel('Score')\n",
    "    ax.legend(title='Text Language', loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba72922a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a melt of metrics for faceted visualization\n",
    "# metrics_df = results.melt(\n",
    "#     id_vars=['technique_id', 'text_lang'],\n",
    "#     value_vars=['accuracy', 'precision', 'recall', 'f1'],\n",
    "#     var_name='metric',\n",
    "#     value_name='score'\n",
    "# )\n",
    "\n",
    "# # Plot\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.barplot(data=metrics_df, x='technique_id', y='score', hue='text_lang', palette='muted', errorbar=None)\n",
    "# plt.title('Metrics by Technique and Language')\n",
    "# plt.ylim(0, 1.05)\n",
    "# plt.ylabel('Score')\n",
    "# plt.xlabel('Technique ID')\n",
    "# plt.legend(title='Text Language')\n",
    "# plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4986bc32",
   "metadata": {},
   "source": [
    "# total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3090eea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "acc = accuracy_score(df['gold'], df['pred'])\n",
    "precision = precision_score(df['gold'], df['pred'], zero_division=0)\n",
    "recall = recall_score(df['gold'], df['pred'], average='binary')\n",
    "f1 = f1_score(df['gold'], df['pred'], average='binary')\n",
    "\n",
    "print(f\"Accuracy: {acc:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-score: {f1:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MS FactUE)",
   "language": "python",
   "name": "factue"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
