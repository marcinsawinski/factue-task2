{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc303d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wd = '/mnt/openfact/users/msawinski/factue-task2'\n",
    "wd = '/Users/marcinsawinski/Documents/GitHub/factue-task2'\n",
    "import sys, os\n",
    "os.chdir(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa931db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Name_Calling-Labeling\"\n",
      "\"Guilt_by_Association\"\n",
      "\"Doubt\"\n",
      "\"Appeal_to_Hypocrisy\"\n",
      "\"Questioning_the_Reputation\"\n",
      "\"Flag_Waving\"\n",
      "\"Appeal_to_Authority\"\n",
      "\"Appeal_to_Popularity\"\n",
      "\"Appeal_to_Values\"\n",
      "\"Appeal_to_Fear-Prejudice\"\n",
      "\"Straw_Man\"\n",
      "\"Red_Herring\"\n",
      "\"Whataboutism\"\n",
      "\"Appeal_to_Pity\"\n",
      "\"Causal_Oversimplification\"\n",
      "\"False_Dilemma-No_Choice\"\n",
      "\"Consequential_Oversimplification\"\n",
      "\"False_Equivalence\"\n",
      "\"Slogans\"\n",
      "\"Conversation_Killer\"\n",
      "\"Appeal_to_Time\"\n",
      "\"Loaded_Language\"\n",
      "\"Obfuscation-Vagueness-Confusion\"\n",
      "\"Exaggeration-Minimisation\"\n",
      "\"Repetition\"\n"
     ]
    }
   ],
   "source": [
    "from factue.methods.llm_calls import load_template_list\n",
    "for x in load_template_list(job='persuasion', step=\"detect\", prompt_version='v001').keys():\n",
    "    print(f'\"{x}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "babd803f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "root = Path(\"data/llm_output/persuasion\")\n",
    "# Example path template\n",
    "files = root.rglob(\"*/*.parquet\")\n",
    "\n",
    "# Read all files and add file path as a column\n",
    "df_list = []\n",
    "for f in files:\n",
    "    df_part = pd.read_parquet(f)\n",
    "    df_part['source_file'] = f  # add the file path\n",
    "    df_list.append(df_part)\n",
    "\n",
    "# Combine into one DataFrame\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "df['split'] = df.source_file.astype(str).str.split('/',expand=True)[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb00c039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>text_lang</th>\n",
       "      <th>text</th>\n",
       "      <th>label_bin</th>\n",
       "      <th>label_multi</th>\n",
       "      <th>base_split</th>\n",
       "      <th>Questioning_the_Reputation</th>\n",
       "      <th>Name_Calling-Labeling</th>\n",
       "      <th>...</th>\n",
       "      <th>description</th>\n",
       "      <th>verdict</th>\n",
       "      <th>is_valid</th>\n",
       "      <th>extra_properties</th>\n",
       "      <th>illegal_value</th>\n",
       "      <th>pred</th>\n",
       "      <th>gold</th>\n",
       "      <th>source_file</th>\n",
       "      <th>error</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pl_eu_12_06_2024_n01.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "      <td>PL</td>\n",
       "      <td>Wicemarszałek Włodzimierz Czarzasty:\\nBardzo p...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[The example does not contain Appeal to Popula...</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>[None, None, None, None, None]</td>\n",
       "      <td>[None, None, None, None, None]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>data/llm_output/persuasion/detect/LLAMA_31_8B/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pl_defence_22_05_2024_n01.txt</td>\n",
       "      <td>3608</td>\n",
       "      <td>3857</td>\n",
       "      <td>PL</td>\n",
       "      <td>Pan wspominał o ABCS – Integrated Battle Comma...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[The example contains Appeal to Popularity bec...</td>\n",
       "      <td>[1, 0, 0, 1, 1]</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>[None, None, None, None, None]</td>\n",
       "      <td>[None, None, None, None, None]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>data/llm_output/persuasion/detect/LLAMA_31_8B/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pl_current_affairs_09_01_2025_n03.txt</td>\n",
       "      <td>1808</td>\n",
       "      <td>1984</td>\n",
       "      <td>PL</td>\n",
       "      <td>(Poseł Anna Gembicka: Decyzja została zmienion...</td>\n",
       "      <td>True</td>\n",
       "      <td>[Repetition]</td>\n",
       "      <td>train</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[The text does not explicitly contain Appeal t...</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>[None, None, None, None, None]</td>\n",
       "      <td>[None, None, None, None, None]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>data/llm_output/persuasion/detect/LLAMA_31_8B/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                filename  start   end text_lang  \\\n",
       "0               pl_eu_12_06_2024_n01.txt      0   135        PL   \n",
       "1          pl_defence_22_05_2024_n01.txt   3608  3857        PL   \n",
       "2  pl_current_affairs_09_01_2025_n03.txt   1808  1984        PL   \n",
       "\n",
       "                                                text  label_bin   label_multi  \\\n",
       "0  Wicemarszałek Włodzimierz Czarzasty:\\nBardzo p...      False            []   \n",
       "1  Pan wspominał o ABCS – Integrated Battle Comma...      False            []   \n",
       "2  (Poseł Anna Gembicka: Decyzja została zmienion...       True  [Repetition]   \n",
       "\n",
       "  base_split Questioning_the_Reputation Name_Calling-Labeling  ...  \\\n",
       "0      train                         []                    []  ...   \n",
       "1      train                         []                    []  ...   \n",
       "2      train                         []                    []  ...   \n",
       "\n",
       "                                         description          verdict  \\\n",
       "0  [The example does not contain Appeal to Popula...  [0, 0, 0, 0, 0]   \n",
       "1  [The example contains Appeal to Popularity bec...  [1, 0, 0, 1, 1]   \n",
       "2  [The text does not explicitly contain Appeal t...  [0, 0, 0, 0, 0]   \n",
       "\n",
       "                         is_valid                extra_properties  \\\n",
       "0  [True, True, True, True, True]  [None, None, None, None, None]   \n",
       "1  [True, True, True, True, True]  [None, None, None, None, None]   \n",
       "2  [True, True, True, True, True]  [None, None, None, None, None]   \n",
       "\n",
       "                    illegal_value pred gold  \\\n",
       "0  [None, None, None, None, None]    0    0   \n",
       "1  [None, None, None, None, None]    1    0   \n",
       "2  [None, None, None, None, None]    0    0   \n",
       "\n",
       "                                         source_file error  split  \n",
       "0  data/llm_output/persuasion/detect/LLAMA_31_8B/...   NaN  train  \n",
       "1  data/llm_output/persuasion/detect/LLAMA_31_8B/...   NaN  train  \n",
       "2  data/llm_output/persuasion/detect/LLAMA_31_8B/...   NaN  train  \n",
       "\n",
       "[3 rows x 57 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e306ff39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pred\n",
       " 0    9728\n",
       " 1    8382\n",
       "-1     143\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.pred.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "657dc767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>text_lang</th>\n",
       "      <th>model_name</th>\n",
       "      <th>split</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Appeal_to_Authority</td>\n",
       "      <td>BG</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>dev</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Appeal_to_Authority</td>\n",
       "      <td>BG</td>\n",
       "      <td>llama3.1:8b</td>\n",
       "      <td>dev</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Appeal_to_Authority</td>\n",
       "      <td>PL</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>dev</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Appeal_to_Authority</td>\n",
       "      <td>PL</td>\n",
       "      <td>llama3.1:8b</td>\n",
       "      <td>dev</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Appeal_to_Authority</td>\n",
       "      <td>RU</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>dev</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Straw_Man</td>\n",
       "      <td>SI</td>\n",
       "      <td>llama3.1:8b</td>\n",
       "      <td>dev</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Whataboutism</td>\n",
       "      <td>BG</td>\n",
       "      <td>llama3.1:8b</td>\n",
       "      <td>dev</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Whataboutism</td>\n",
       "      <td>PL</td>\n",
       "      <td>llama3.1:8b</td>\n",
       "      <td>dev</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Whataboutism</td>\n",
       "      <td>RU</td>\n",
       "      <td>llama3.1:8b</td>\n",
       "      <td>dev</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Whataboutism</td>\n",
       "      <td>SI</td>\n",
       "      <td>llama3.1:8b</td>\n",
       "      <td>dev</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             prompt_name text_lang   model_name split  filename\n",
       "0    Appeal_to_Authority        BG  gpt-4o-mini   dev       184\n",
       "1    Appeal_to_Authority        BG  llama3.1:8b   dev       164\n",
       "2    Appeal_to_Authority        PL  gpt-4o-mini   dev       114\n",
       "3    Appeal_to_Authority        PL  llama3.1:8b   dev       114\n",
       "4    Appeal_to_Authority        RU  gpt-4o-mini   dev        90\n",
       "..                   ...       ...          ...   ...       ...\n",
       "107            Straw_Man        SI  llama3.1:8b   dev        41\n",
       "108         Whataboutism        BG  llama3.1:8b   dev       164\n",
       "109         Whataboutism        PL  llama3.1:8b   dev       114\n",
       "110         Whataboutism        RU  llama3.1:8b   dev        90\n",
       "111         Whataboutism        SI  llama3.1:8b   dev        41\n",
       "\n",
       "[112 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.split=='dev'].groupby([\"prompt_name\",\"text_lang\",\"model_name\",\"split\"])['filename'].agg(\"count\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb44b23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_binary(x):\n",
    "    return 1 if str(x).strip().lower() in {'1', 'true'} else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3da0615",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['source_file'].value_counts().sort_index()\n",
    "df['gold'] = df['gold'].apply(normalize_binary)\n",
    "df['pred'] = df['pred'].apply(normalize_binary)\n",
    "df['split'] = df.source_file.astype(str).str.split('/',expand=True)[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ae9f572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prompt_name\n",
       "Appeal_to_Popularity          1487\n",
       "Doubt                         1487\n",
       "Appeal_to_Hypocrisy           1477\n",
       "Appeal_to_Authority           1477\n",
       "Flag_Waving                   1467\n",
       "Name_Calling-Labeling         1467\n",
       "Appeal_to_Fear-Prejudice      1427\n",
       "Appeal_to_Values              1407\n",
       "Straw_Man                     1266\n",
       "Guilt_by_Association          1149\n",
       "Whataboutism                  1058\n",
       "Red_Herring                   1018\n",
       "Questioning_the_Reputation     668\n",
       "Exaggeration-Minimisation      429\n",
       "Repetition                     429\n",
       "Loaded_Language                419\n",
       "Appeal_to_Pity                 121\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.prompt_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "228d066e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>text_lang</th>\n",
       "      <th>model_name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Appeal_to_Popularity</td>\n",
       "      <td>PL</td>\n",
       "      <td>llama3.1:8b</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.24</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Appeal_to_Popularity</td>\n",
       "      <td>PL</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.31</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Appeal_to_Popularity</td>\n",
       "      <td>RU</td>\n",
       "      <td>llama3.1:8b</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Appeal_to_Popularity</td>\n",
       "      <td>RU</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.29</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Appeal_to_Popularity</td>\n",
       "      <td>SI</td>\n",
       "      <td>llama3.1:8b</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.44</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Repetition</td>\n",
       "      <td>BG</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.13</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Loaded_Language</td>\n",
       "      <td>PL</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.50</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Loaded_Language</td>\n",
       "      <td>RU</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.10</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Loaded_Language</td>\n",
       "      <td>SI</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.69</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Loaded_Language</td>\n",
       "      <td>BG</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.50</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              prompt_name text_lang   model_name  accuracy  precision  recall  \\\n",
       "0    Appeal_to_Popularity        PL  llama3.1:8b      0.63       0.15    0.66   \n",
       "1    Appeal_to_Popularity        PL  gpt-4o-mini      0.84       0.36    0.27   \n",
       "2    Appeal_to_Popularity        RU  llama3.1:8b      0.42       0.02    1.00   \n",
       "3    Appeal_to_Popularity        RU  gpt-4o-mini      0.94       0.17    1.00   \n",
       "4    Appeal_to_Popularity        SI  llama3.1:8b      0.62       0.30    0.87   \n",
       "..                    ...       ...          ...       ...        ...     ...   \n",
       "107            Repetition        BG  gpt-4o-mini      0.64       0.07    0.83   \n",
       "108       Loaded_Language        PL  gpt-4o-mini      0.55       0.34    0.93   \n",
       "109       Loaded_Language        RU  gpt-4o-mini      0.35       0.06    0.60   \n",
       "110       Loaded_Language        SI  gpt-4o-mini      0.76       0.55    0.92   \n",
       "111       Loaded_Language        BG  gpt-4o-mini      0.75       0.34    0.92   \n",
       "\n",
       "       f1  support  \n",
       "0    0.24      316  \n",
       "1    0.31      114  \n",
       "2    0.04      257  \n",
       "3    0.29       90  \n",
       "4    0.44       87  \n",
       "..    ...      ...  \n",
       "107  0.13      184  \n",
       "108  0.50      114  \n",
       "109  0.10       80  \n",
       "110  0.69       41  \n",
       "111  0.50      184  \n",
       "\n",
       "[112 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "# Define a function to compute metrics for a group\n",
    "# Assume df has columns: prompt_name, text_lang, gold, pred\n",
    "rows = []\n",
    "\n",
    "def is_valid_label(x):\n",
    "    return x in (0, 1)\n",
    "\n",
    "rows = []\n",
    "\n",
    "for prompt_name in df['prompt_name'].unique():\n",
    "    for text_lang in df.loc[df['prompt_name'] == prompt_name, 'text_lang'].unique():\n",
    "        for model_name in df.loc[(df['prompt_name'] == prompt_name) & (df['text_lang'] == text_lang), 'model_name'].unique():\n",
    "            group = df[\n",
    "                (df['prompt_name'] == prompt_name) &\n",
    "                (df['text_lang'] == text_lang) &\n",
    "                (df['model_name'] == model_name)\n",
    "            ]\n",
    "            if len(group) > 0:\n",
    "                row = {\n",
    "                    'prompt_name': prompt_name,\n",
    "                    'text_lang': text_lang,\n",
    "                    'model_name': model_name,\n",
    "                    'accuracy': accuracy_score(group['gold'], group['pred']),\n",
    "                    'precision': precision_score(group['gold'], group['pred'], zero_division=0),\n",
    "                    'recall': recall_score(group['gold'], group['pred'], zero_division=0),\n",
    "                    'f1': f1_score(group['gold'], group['pred'], zero_division=0),\n",
    "                    'support': len(group)\n",
    "                }\n",
    "                rows.append(row)\n",
    "\n",
    "results = pd.DataFrame(rows)\n",
    "results[['accuracy', 'precision','recall', 'f1']] = results[['accuracy', 'precision','recall', 'f1']].round(2)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "879f3744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea77b86",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba72922a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'split'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Melt the results dataframe\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m metrics_df \u001b[38;5;241m=\u001b[39m results[\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msplit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmelt(\n\u001b[1;32m      6\u001b[0m     id_vars\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_lang\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      7\u001b[0m     value_vars\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      8\u001b[0m     var_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m     value_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Create FacetGrid\u001b[39;00m\n\u001b[1;32m     13\u001b[0m g \u001b[38;5;241m=\u001b[39m sns\u001b[38;5;241m.\u001b[39mFacetGrid(\n\u001b[1;32m     14\u001b[0m     metrics_df, \n\u001b[1;32m     15\u001b[0m     row\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_lang\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     aspect\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.5\u001b[39m\n\u001b[1;32m     21\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Documents/GitHub/factue-task2/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'split'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Melt the results dataframe\n",
    "metrics_df = results[results['split']=='train'].melt(\n",
    "    id_vars=['prompt_name', 'text_lang', 'model_name'],\n",
    "    value_vars=['accuracy', 'precision', 'recall', 'f1'],\n",
    "    var_name='metric',\n",
    "    value_name='score'\n",
    ")\n",
    "\n",
    "# Create FacetGrid\n",
    "g = sns.FacetGrid(\n",
    "    metrics_df, \n",
    "    row=\"text_lang\", \n",
    "    col=\"metric\", \n",
    "    sharex=True, \n",
    "    sharey=False, \n",
    "    height=4, \n",
    "    aspect=1.5\n",
    ")\n",
    "\n",
    "# Use map_dataframe and assign `hue` in the barplot call\n",
    "g.map_dataframe(\n",
    "    sns.barplot, \n",
    "    x=\"score\", \n",
    "    y=\"prompt_name\", \n",
    "    hue=\"model_name\", \n",
    "    errorbar=None, \n",
    "    palette='muted',\n",
    "    legend=False  # Disable auto-legend here\n",
    ")\n",
    "\n",
    "# Add a single legend to the full figure\n",
    "g.add_legend(title='Model')\n",
    "g.set_axis_labels(\"Score\", \"Technique ID\")\n",
    "g.set_titles(row_template=\"{row_name}\", col_template=\"{col_name}\")\n",
    "for ax in g.axes.flatten():\n",
    "    ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4986bc32",
   "metadata": {},
   "source": [
    "# total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3090eea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.588\n",
      "Precision: 0.125\n",
      "Recall: 0.848\n",
      "F1-score: 0.218\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "acc = accuracy_score(df['gold'], df['pred'])\n",
    "precision = precision_score(df['gold'], df['pred'], zero_division=0)\n",
    "recall = recall_score(df['gold'], df['pred'], average='binary')\n",
    "f1 = f1_score(df['gold'], df['pred'], average='binary')\n",
    "\n",
    "print(f\"Accuracy: {acc:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-score: {f1:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
