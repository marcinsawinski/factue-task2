{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "25665ab6",
            "metadata": {},
            "outputs": [],
            "source": [
                "from factue.utils.types import ModelMode, ModelName, ModelProvider"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "9dcc81bb",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<ModelName.LLAMA_31_8B: 'llama3.1:8b'>"
                        ]
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "x = ModelName(\"llama3.1:8b\")\n",
                "x"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "7dc98259",
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "07ece01e",
            "metadata": {},
            "outputs": [
                {
                    "ename": "ModuleNotFoundError",
                    "evalue": "No module named 'factue.pipelines.clean_groundtruth'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mluigi\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfactue\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpipelines\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclean_groundtruth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CleanGroundTruthTask\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfactue\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvars\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PROJECT_ROOT\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfactue\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mviz\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m disp\n",
                        "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'factue.pipelines.clean_groundtruth'"
                    ]
                }
            ],
            "source": [
                "import luigi\n",
                "import pandas as pd\n",
                "from factue.pipelines.clean_groundtruth import CleanGroundTruthTask\n",
                "from factue.utils.vars import PROJECT_ROOT\n",
                "from factue.utils.viz import disp"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9a345ed5",
            "metadata": {},
            "outputs": [],
            "source": [
                "sample_in = (\n",
                "    PROJECT_ROOT / \"data/01_preprocessed\" / \"dev\" / \"dev-eng\" / \"batch_0000.parquet\"\n",
                ")\n",
                "df = pd.read_parquet(sample_in)\n",
                "df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d633d359",
            "metadata": {},
            "outputs": [],
            "source": [
                "t1 = CleanGroundTruthTask(\n",
                "    input_path=sample_in,\n",
                "    input_dir=PROJECT_ROOT / \"data/01_preprocessed\",\n",
                "    output_dir=PROJECT_ROOT / \"data/02_cleaned_ground_truth\",\n",
                "    force=True,\n",
                ")\n",
                "\n",
                "luigi.build([t1], local_scheduler=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9e55e7e0",
            "metadata": {},
            "outputs": [],
            "source": [
                "sample_out = (\n",
                "    PROJECT_ROOT\n",
                "    / \"data/02_cleaned_ground_truth\"\n",
                "    / \"dev\"\n",
                "    / \"dev-eng\"\n",
                "    / \"batch_0000.parquet\"\n",
                ")\n",
                "df = pd.read_parquet(sample_out)\n",
                "df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ba3572dd",
            "metadata": {},
            "outputs": [],
            "source": [
                "# from langchain.chat_models import ChatOpenAI\n",
                "# from langchain.schema import SystemMessage, HumanMessage\n",
                "# from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
                "# from langchain.output_parsers import JsonOutputKeyToolsParser\n",
                "\n",
                "# # 1. Set up the system prompt\n",
                "# system_prompt = SystemMessage(\n",
                "#     \"You are an expert journalist and fact-checker. \"\n",
                "#     \"You compare extracted claims with source posts to verify accuracy. \"\n",
                "#     \"Return only structured JSON that answers five fact-checking questions. \"\n",
                "#     \"Do not explain or output anything outside of the JSON structure.\"\n",
                "# )\n",
                "\n",
                "# # 2. Human prompt template with variables\n",
                "# human_prompt_template = HumanMessagePromptTemplate.from_template(\n",
                "#     \"\"\"\n",
                "# You are an expert journalist and fact-checker. Your task is to compare an extracted claim with the original post and determine whether the claim accurately reflects the source content.\n",
                "\n",
                "# For each of the following questions, answer `true` or `false` based on your analysis. If your answer is `false`, provide a list of specific errors. If `true`, keep the list empty.\n",
                "\n",
                "# Strictly respond in the following JSON format:\n",
                "\n",
                "# {{\n",
                "#   \"questions\": [\n",
                "#     {{\n",
                "#       \"id\": \"meaning_preserved\",\n",
                "#       \"answer\": true | false,\n",
                "#       \"errors\": [ \"...\" ]\n",
                "#     }},\n",
                "#     {{\n",
                "#       \"id\": \"correct_named_entities\",\n",
                "#       \"answer\": true | false,\n",
                "#       \"errors\": [ \"...\" ]\n",
                "#     }},\n",
                "#     {{\n",
                "#       \"id\": \"correct_numbers\",\n",
                "#       \"answer\": true | false,\n",
                "#       \"errors\": [ \"...\" ]\n",
                "#     }},\n",
                "#     {{\n",
                "#       \"id\": \"no_added_data\",\n",
                "#       \"answer\": true | false,\n",
                "#       \"errors\": [ \"...\" ]\n",
                "#     }},\n",
                "#     {{\n",
                "#       \"id\": \"no_missing_data\",\n",
                "#       \"answer\": true | false,\n",
                "#       \"errors\": [ \"...\" ]\n",
                "#     }}\n",
                "#   ]\n",
                "# }}\n",
                "\n",
                "# ---\n",
                "\n",
                "# CLAIM:\n",
                "# \\\"\\\"\\\"{claim}\\\"\\\"\\\"\n",
                "\n",
                "# POST:\n",
                "# \\\"\\\"\\\"{post}\\\"\\\"\\\"\n",
                "# \"\"\"\n",
                "# )\n",
                "\n",
                "# # 3. Build the complete prompt\n",
                "# prompt = ChatPromptTemplate.from_messages([\n",
                "#     system_prompt,\n",
                "#     human_prompt_template\n",
                "# ])\n",
                "\n",
                "# # 4. Define the model\n",
                "# # Example: If you are using LM Studio, set the correct base_url and API key (fake one)\n",
                "# llm = ChatOpenAI(\n",
                "#     model=\"local-llm\",  # or \"gpt-3.5-turbo\" or any other\n",
                "#     openai_api_base=\"http://localhost:1234/v1\",  # LM Studio endpoint\n",
                "#     openai_api_key=\"lm-studio\",  # dummy key if local\n",
                "#     temperature=0.0\n",
                "# )\n",
                "\n",
                "# # 5. Output parser\n",
                "# parser = JsonOutputKeyToolsParser()\n",
                "\n",
                "# # 6. LangChain call function\n",
                "# def factcheck_claim_against_post(claim: str, post: str) -> dict:\n",
                "#     messages = prompt.format_prompt(claim=claim, post=post).to_messages()\n",
                "#     response = llm(messages)\n",
                "#     result = parser.parse(response)\n",
                "#     return result\n",
                "\n",
                "# # 7. Example usage\n",
                "# if __name__ == \"__main__\":\n",
                "#     claim = \"The president announced 5 billion dollars to rebuild Rome in 2025.\"\n",
                "#     post = \"In 2025, the mayor of Florence announced 500 million dollars for road repairs.\"\n",
                "\n",
                "#     output = factcheck_claim_against_post(claim, post)\n",
                "#     print(output)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4f725b92",
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d143d96e",
            "metadata": {},
            "outputs": [],
            "source": [
                "from factue.methods.llm_call import validate_claim"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b9c351e0",
            "metadata": {},
            "outputs": [],
            "source": [
                "claim = \"\"\"Photo shows Louis Armstrong as a child\"\"\"\n",
                "post = \"\"\"The Karnofsky Jewish family, who immigrated to the United States from Lithuania, employed a 7-year-old boy and adopted (so to speak) him into their home.  He was originally given homework to get food because he was a starving kid.  He remained under the Jewish families employ, until he was 12  Karnofsky gave him money to buy his first instrument, which was a common instrument in Jewish families.  They really admired his musical talent.Later, when he became a professional … See More The Karnofsky Jewish family, who immigrated to the United States from Lithuania, employed a 7-year-old boy and adopted (so to speak) him into their home.  He was originally given homework to get food because he was a starving kid.  He remained under the Jewish families employ, until he was 12  Karnofsky gave him money to buy his first instrument, which was a common instrument in Jewish families.  They really admired his musical talent.Later, when he became a professional … See More The Karnofsky Jewish family, who immigrated to the United States from Lithuania, employed a 7-year-old boy and adopted (so to speak) him into their home.  He was originally given homework to get food because he was a starving kid.  He remained under the Jewish families employ, until he was 12  Karnofsky gave him money to buy his first instrument, which was a common instrument in Jewish families.  They really admired his musical talent.Later, when he became a professional … See More None\"\"\"\n",
                "print(validate_claim(\"v001\", \"v001\", post, claim))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "57289a48",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_core.messages import HumanMessage\n",
                "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
                "from factue.methods.prompts.check_claim import (\n",
                "    check_claim_prompt_templates,\n",
                "    check_claim_prompt_formating,\n",
                ")\n",
                "\n",
                "# from factue.methods.llm_langchain.lms import init_lms\n",
                "from factue.methods.llm_langchain.ollama import init_ollama\n",
                "\n",
                "# \"llama3.1:8b\"\n",
                "# \"deepseek-r1:8b\"\n",
                "llm = init_ollama(model=\"llama3.1:8b\", temperature=0.0)\n",
                "\n",
                "\n",
                "# Function to run on each row\n",
                "def validate_claim(prompt_version, format_version, post, claim):\n",
                "    human_prompt_template = HumanMessagePromptTemplate.from_template(\n",
                "        check_claim_prompt_templates[prompt_version]\n",
                "    )\n",
                "    format = check_claim_prompt_formating[format_version]\n",
                "    prompt = human_prompt_template.format(claim=claim, post=post, format=format)\n",
                "    print(prompt.content)\n",
                "    # ai_msg = llm.invoke([prompt])\n",
                "    # return ai_msg.content.strip()\n",
                "\n",
                "\n",
                "claim = \"\"\"Photo shows Louis Armstrong as a child\"\"\"\n",
                "post = \"\"\"The Karnofsky Jewish family, who immigrated to the United States from Lithuania, employed a 7-year-old boy and adopted (so to speak) him into their home.  He was originally given homework to get food because he was a starving kid.  He remained under the Jewish families employ, until he was 12  Karnofsky gave him money to buy his first instrument, which was a common instrument in Jewish families.  They really admired his musical talent.Later, when he became a professional … See More The Karnofsky Jewish family, who immigrated to the United States from Lithuania, employed a 7-year-old boy and adopted (so to speak) him into their home.  He was originally given homework to get food because he was a starving kid.  He remained under the Jewish families employ, until he was 12  Karnofsky gave him money to buy his first instrument, which was a common instrument in Jewish families.  They really admired his musical talent.Later, when he became a professional … See More The Karnofsky Jewish family, who immigrated to the United States from Lithuania, employed a 7-year-old boy and adopted (so to speak) him into their home.  He was originally given homework to get food because he was a starving kid.  He remained under the Jewish families employ, until he was 12  Karnofsky gave him money to buy his first instrument, which was a common instrument in Jewish families.  They really admired his musical talent.Later, when he became a professional … See More None\"\"\"\n",
                "print(validate_claim(\"v001\", \"v001\", post, claim))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "acd8c367",
            "metadata": {},
            "source": [
                "# remote llama"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c5d876f2",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7fb1169e",
            "metadata": {},
            "outputs": [],
            "source": [
                "from factue.methods.llm_langchain.ollama import init_ollama\n",
                "\n",
                "# \"llama3.1:8b\"\n",
                "# \"deepseek-r1:8b\"\n",
                "llm = init_ollama(model=\"llama3.1:8b\", temperature=0.0)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f3a64115",
            "metadata": {},
            "outputs": [],
            "source": [
                "llm.invoke(\n",
                "    '''You are an expert journalist and fact-checker. Your task is to compare an extracted claim with the original post and determine whether the claim accurately reflects the source content.\n",
                "For each of the following questions, answer `true` or `false` based on your analysis. If your answer is `false`, provide a list of specific errors. If `true`, keep the list empty.\n",
                "\n",
                "\n",
                "Strictly respond in the following JSON format:\n",
                "{{\n",
                "  \"questions\": [\n",
                "    {{\n",
                "      \"id\": \"meaning_preserved\",\n",
                "      \"answer\": true | false,\n",
                "      \"errors\": [ \"...\" ]\n",
                "    }},\n",
                "    {{\n",
                "      \"id\": \"correct_named_entities\",\n",
                "      \"answer\": true | false,\n",
                "      \"errors\": [ \"...\" ]\n",
                "    }},\n",
                "    {{\n",
                "      \"id\": \"correct_numbers\",\n",
                "      \"answer\": true | false,\n",
                "      \"errors\": [ \"...\" ]\n",
                "    }},\n",
                "    {{\n",
                "      \"id\": \"no_added_data\",\n",
                "      \"answer\": true | false,\n",
                "      \"errors\": [ \"...\" ]\n",
                "    }},\n",
                "    {{\n",
                "      \"id\": \"no_missing_data\",\n",
                "      \"answer\": true | false,\n",
                "      \"errors\": [ \"...\" ]\n",
                "    }}\n",
                "  ]\n",
                "}}\n",
                "\n",
                "CLAIM:\n",
                "\"\"\"Photo shows Louis Armstrong as a child\"\"\"\n",
                "\n",
                "POST:\n",
                "\"\"\"The Karnofsky Jewish family, who immigrated to the United States from Lithuania, employed a 7-year-old boy and adopted (so to speak) him into their home.  He was originally given homework to get food because he was a starving kid.  He remained under the Jewish families employ, until he was 12  Karnofsky gave him money to buy his first instrument, which was a common instrument in Jewish families.  They really admired his musical talent.Later, when he became a professional … See More The Karnofsky Jewish family, who immigrated to the United States from Lithuania, employed a 7-year-old boy and adopted (so to speak) him into their home.  He was originally given homework to get food because he was a starving kid.  He remained under the Jewish families employ, until he was 12  Karnofsky gave him money to buy his first instrument, which was a common instrument in Jewish families.  They really admired his musical talent.Later, when he became a professional … See More The Karnofsky Jewish family, who immigrated to the United States from Lithuania, employed a 7-year-old boy and adopted (so to speak) him into their home.  He was originally given homework to get food because he was a starving kid.  He remained under the Jewish families employ, until he was 12  Karnofsky gave him money to buy his first instrument, which was a common instrument in Jewish families.  They really admired his musical talent.Later, when he became a professional … See More None”””'''\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2a5f71ec",
            "metadata": {},
            "source": [
                "# previous cell"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e95be51a",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_core.messages import HumanMessage\n",
                "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
                "from factue.methods.prompts.check_claim import templates_check_claim\n",
                "\n",
                "# from factue.methods.llm_langchain.lms import init_lms\n",
                "from factue.methods.llm_langchain.ollama import init_ollama\n",
                "\n",
                "llm = init_ollama()\n",
                "\n",
                "\n",
                "# Function to run on each row\n",
                "def validate_claim(version, post, claim):\n",
                "    human_prompt_template = HumanMessagePromptTemplate.from_template(\n",
                "        templates_check_claim[version]\n",
                "    )\n",
                "    prompt = human_prompt_template.format(claim=claim, post=post)\n",
                "    print(prompt.content)\n",
                "    # ai_msg = llm.invoke([prompt])\n",
                "    # return ai_msg.content.strip()\n",
                "\n",
                "\n",
                "claim = \"\"\"Photo shows Louis Armstrong as a child\"\"\"\n",
                "post = \"\"\"The Karnofsky Jewish family, who immigrated to the United States from Lithuania, employed a 7-year-old boy and adopted (so to speak) him into their home.  He was originally given homework to get food because he was a starving kid.  He remained under the Jewish families employ, until he was 12  Karnofsky gave him money to buy his first instrument, which was a common instrument in Jewish families.  They really admired his musical talent.Later, when he became a professional … See More The Karnofsky Jewish family, who immigrated to the United States from Lithuania, employed a 7-year-old boy and adopted (so to speak) him into their home.  He was originally given homework to get food because he was a starving kid.  He remained under the Jewish families employ, until he was 12  Karnofsky gave him money to buy his first instrument, which was a common instrument in Jewish families.  They really admired his musical talent.Later, when he became a professional … See More The Karnofsky Jewish family, who immigrated to the United States from Lithuania, employed a 7-year-old boy and adopted (so to speak) him into their home.  He was originally given homework to get food because he was a starving kid.  He remained under the Jewish families employ, until he was 12  Karnofsky gave him money to buy his first instrument, which was a common instrument in Jewish families.  They really admired his musical talent.Later, when he became a professional … See More None\"\"\"\n",
                "print(validate_claim(\"v003\", post, claim))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "11fb7900",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8972c39b",
            "metadata": {},
            "outputs": [],
            "source": [
                "df[\"validation\"] = df.apply(lambda x: validate_claim(x[\"post\"], x[\"gold\"]), axis=1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a5540508",
            "metadata": {},
            "outputs": [],
            "source": [
                "disp(df[[\"post\", \"gold\", \"validation\"]])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cf57a33c",
            "metadata": {},
            "source": [
                "# old\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2ec719a8",
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import requests\n",
                "\n",
                "\n",
                "def compare_claim_with_post(claim_text: str, original_post: str) -> dict:\n",
                "    prompt = f\"\"\"\n",
                "You are a fact-checking assistant.\n",
                "\n",
                "Compare the claim with the original post, and determine if the claim is factually correct. If any information is wrong, missing, or added, return the issues categorized under these keys:\n",
                "- \"wrong_meaning\": if the claim has a different meaning than the original post.\n",
                "- \"wrong_name\": if people, places, or organizations are misidentified.\n",
                "- \"wrong_number\": if numeric values (e.g. money, percentages, dates) are incorrect.\n",
                "- \"missing_data\": if important details from the original post are not mentioned in the claim.\n",
                "- \"added_data\": if the claim includes things not present in the original.\n",
                "\n",
                "Return ONLY a valid JSON object like this:\n",
                "\n",
                "{{\n",
                "  \"is_factually_correct\": true | false,\n",
                "  \"errors\": {{\n",
                "    \"wrong_meaning\": [ \"...\", \"...\" ],\n",
                "    \"wrong_name\": [ \"...\", \"...\" ],\n",
                "    \"wrong_number\": [ \"...\", \"...\" ],\n",
                "    \"missing_data\": [ \"...\", \"...\" ],\n",
                "    \"added_data\": [ \"...\", \"...\" ]\n",
                "  }}\n",
                "}}\n",
                "\n",
                "CLAIM:\n",
                "\\\"\\\"\\\"{claim_text}\\\"\\\"\\\"\n",
                "\n",
                "ORIGINAL POST:\n",
                "\\\"\\\"\\\"{original_post}\\\"\\\"\\\"\n",
                "\n",
                "Do not explain or include anything else — just the JSON.\n",
                "\"\"\"\n",
                "\n",
                "    response = requests.post(\n",
                "        \"http://localhost:1234/v1/completions\",\n",
                "        headers={\"Content-Type\": \"application/json\"},\n",
                "        data=json.dumps(\n",
                "            {\"prompt\": prompt, \"max_tokens\": 512, \"temperature\": 0.2, \"stop\": [\"\\n\\n\"]}\n",
                "        ),\n",
                "    )\n",
                "\n",
                "    try:\n",
                "        return json.loads(response.json()[\"choices\"][0][\"text\"].strip())\n",
                "    except Exception as e:\n",
                "        return {\n",
                "            \"is_factually_correct\": False,\n",
                "            \"errors\": {\n",
                "                \"wrong_name\": [],\n",
                "                \"wrong_number\": [],\n",
                "                \"missing_data\": [],\n",
                "                \"added_data\": [],\n",
                "                \"parsing_error\": [f\"Model response parsing failed: {str(e)}\"],\n",
                "            },\n",
                "        }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bd9fe167",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_core.messages import HumanMessage, SystemMessage\n",
                "\n",
                "\n",
                "system_prompt = SystemMessage(\n",
                "    \"You are an expert journalist that checks for a most concise formulation of possibly false claims extracted from the post. \"\n",
                "    \"Compare the claim with the post and check if the claim is accurately reflecting the post and revise if needed\"\n",
                "    \"Output only the claim. Do not add any additional information. Do not correct the false claim. The claim should be a single sentence. If there is no claim, say 'No claim'.\"\n",
                ")\n",
                "\n",
                "\n",
                "# Function to run on each row\n",
                "def validate_claim(post, claim):\n",
                "    text = f\"Post: {post}\\n\\n Claim: {claim}\"\n",
                "    messages = [\n",
                "        system_prompt,\n",
                "        HumanMessage(content=text),\n",
                "    ]\n",
                "    ai_msg = llm.invoke(messages)\n",
                "    return ai_msg.content.strip()\n",
                "\n",
                "\n",
                "df[\"validation\"] = df.apply(lambda x: validate_claim(x[\"post\"], x[\"gold\"]), axis=1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "728ca95f",
            "metadata": {},
            "outputs": [],
            "source": [
                "disp(df[[\"post\", \"gold\", \"validation\"]])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d6e8db20",
            "metadata": {},
            "outputs": [],
            "source": [
                "disp(df[[\"post\", \"gold\", \"validation\"]])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d433cfe7",
            "metadata": {},
            "outputs": [],
            "source": [
                "schema = \"\"\"\n",
                "{\n",
                "  \"title\": \"FactCheckResult\",\n",
                "  \"type\": \"object\",\n",
                "  \"properties\": {\n",
                "    \"is_factually_correct\": {\n",
                "      \"title\": \"Is Factually Correct\",\n",
                "      \"type\": \"boolean\",\n",
                "      \"description\": \"Indicates whether the extracted claim is factually accurate based on the original post.\",\n",
                "      \"default\": false,\n",
                "      \"examples\": [\n",
                "        true,\n",
                "        false\n",
                "      ]\n",
                "    },\n",
                "    \"errors\": {\n",
                "      \"$ref\": \"#/definitions/ErrorGroups\"\n",
                "    }\n",
                "  },\n",
                "  \"required\": [\n",
                "    \"is_factually_correct\",\n",
                "    \"errors\"\n",
                "  ],\n",
                "  \"propertyOrder\": [\n",
                "    \"is_factually_correct\",\n",
                "    \"errors\"\n",
                "  ],\n",
                "  \"definitions\": {\n",
                "    \"ErrorGroups\": {\n",
                "      \"title\": \"ErrorGroups\",\n",
                "      \"type\": \"object\",\n",
                "      \"description\": \"Grouped lists of factual inconsistencies detected in the claim.\",\n",
                "      \"propertyOrder\": [\n",
                "        \"wrong_name\",\n",
                "        \"wrong_number\",\n",
                "        \"missing_data\",\n",
                "        \"added_data\"\n",
                "      ],\n",
                "      \"properties\": {\n",
                "        \"wrong_name\": {\n",
                "          \"title\": \"Wrong Name\",\n",
                "          \"type\": \"array\",\n",
                "          \"items\": {\n",
                "            \"type\": \"string\"\n",
                "          },\n",
                "          \"description\": \"Names, places, or organizations incorrectly stated in the claim.\",\n",
                "          \"default\": [],\n",
                "          \"examples\": [\n",
                "            [\n",
                "              \"Claim mentions 'Barack Obama' but original post says 'Joe Biden'.\"\n",
                "            ]\n",
                "          ]\n",
                "        },\n",
                "        \"wrong_number\": {\n",
                "          \"title\": \"Wrong Number\",\n",
                "          \"type\": \"array\",\n",
                "          \"items\": {\n",
                "            \"type\": \"string\"\n",
                "          },\n",
                "          \"description\": \"Numerical inaccuracies in the claim compared to the original post.\",\n",
                "          \"default\": [],\n",
                "          \"examples\": [\n",
                "            [\n",
                "              \"Claim says '10,000 people' but original says '1,000'.\"\n",
                "            ]\n",
                "          ]\n",
                "        },\n",
                "        \"missing_data\": {\n",
                "          \"title\": \"Missing Data\",\n",
                "          \"type\": \"array\",\n",
                "          \"items\": {\n",
                "            \"type\": \"string\"\n",
                "          },\n",
                "          \"description\": \"Important context or facts missing from the claim but present in the original.\",\n",
                "          \"default\": [],\n",
                "          \"examples\": [\n",
                "            [\n",
                "              \"Claim omits that the funds were conditional on approval.\"\n",
                "            ]\n",
                "          ]\n",
                "        },\n",
                "        \"added_data\": {\n",
                "          \"title\": \"Added Data\",\n",
                "          \"type\": \"array\",\n",
                "          \"items\": {\n",
                "            \"type\": \"string\"\n",
                "          },\n",
                "          \"description\": \"Information added in the claim that was not mentioned in the original post.\",\n",
                "          \"default\": [],\n",
                "          \"examples\": [\n",
                "            [\n",
                "              \"Claim adds 'military support' while the original only mentions 'infrastructure aid'.\"\n",
                "            ]\n",
                "          ]\n",
                "        }\n",
                "      },\n",
                "      \"required\": [\n",
                "        \"wrong_name\",\n",
                "        \"wrong_number\",\n",
                "        \"missing_data\",\n",
                "        \"added_data\"\n",
                "      ]\n",
                "    }\n",
                "  }\n",
                "}\n",
                "\"\"\""
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}